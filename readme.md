# This is tiny_compass

## ğŸ˜ƒWhy tiny-compass? 
åˆå…¥`LLM`å¤§é—¨ï¼Œä½ æ˜¯å¦æœ‰ç±»ä¼¼çš„å›°æƒ‘:

1. æ¨¡å‹äº”èŠ±å…«é—¨ï¼Œå‚åŸŸä»»åŠ¡ä¹Ÿäº”èŠ±å…«é—¨ã€‚é™¤äº†`human_eval`ä¹‹å¤–ï¼Œå¦‚ä½•å¯¹ä¸ªæ€§åŒ–çš„ä»»åŠ¡æä¾›æœ‰è¯´æœåŠ›çš„å®šé‡æ€§èƒ½æŒ‡æ ‡?  
2. å„ä¸ªæ¨¡å‹çš„è¯„æµ‹æŒ‡æ ‡äº”èŠ±å…«é—¨?å°ç™½åˆå­¦è€…çœ‹ä¸æ‡‚,éš¾ä»¥å­¦ä¹ ?
3. è¯„æµ‹`metric`ä¸ä¼šé€‰,é™¤äº†`rouge`,`blue`æƒ³ä¸åˆ°å…¶ä»–çš„`metric`?
4. æƒ³è®©`LLM`åšé€‰æ‹©é¢˜,ä½†æ˜¯æ¨¡å‹è¾“å‡ºäº†ä¸€å¤§å †,å¦‚ä½•è¯„ä»·é€‰æ‹©èƒ½åŠ›?

å¦‚æœæœ‰ï¼Œé‚£ä¹ˆ:   
<span style="font-size: 24px;">**_tiny-compass is all you need!_**</span>


## ğŸ™‹What is compass?
é¦–å…ˆè¦æ˜ç¡®è¯„æµ‹ä»»åŠ¡çš„åŸºç¡€pipelineã€‚ä¸‹å›¾æ˜¯è¯„æµ‹ä»»åŠ¡çš„ç®€è¦æµç¨‹ï¼š 

![è¯„æµ‹å›¾](./docs/tiny_compass.png)  

- é¦–å…ˆï¼Œæ ¹æ®ç›®æ ‡æ•°æ®é›†çš„ä»»åŠ¡ç±»å‹æŒ‡å®šåˆç†çš„è¯„æµ‹`metric`.
- æ ¹æ®ç›®æ ‡æ•°æ®çš„å½¢å¼æ€»ç»“æ¨¡å‹å¼•å¯¼`prompt`.
- æ ¹æ®æ¨¡å‹åˆæ­¥é¢„æµ‹ç»“æœé‡‡çº³åˆç†çš„æŠ½å–æ–¹å¼.
- å¯¹ç›¸åº”çš„`pred`ä¸`anwser`è¿›è¡Œå¾—åˆ†è®¡ç®—.

## ğŸ˜‹Support datasets&metrics.
æ‰€é‡‡ç”¨çš„æ•°æ®é›†åœ¨è¿™é‡Œ[here](./dataset/),ç›®å‰æœ‰çš„æ•°æ®é›†ä¸ç±»å‹åŒ…å«(åç»­ä¼šæŒç»­æ›´æ–°!): 

|name|type|metric|
|---|---|---|
|multi_news|é•¿æ–‡æœ¬é—®ç­”|Rouge|
|multifieldqa_zh|çŸ­æ–‡æœ¬é—®ç­”|f1|
|trec|ç”Ÿæˆå¼é€‰åˆ™|accuracy|

## ğŸ’Metrics explanation.
çœ‹åˆ°äº†ä¸Šé¢çš„æŒ‡æ ‡æ˜¯å¦æœ‰è¿™æ ·çš„ç–‘é—®:  
- What? `F1` ä¸æ˜¯åˆ†ç±»æŒ‡æ ‡ï¼Œæ€ä¹ˆè·‘`llm`å»äº†?
- `accuracy`ä¸æ˜¯è¦åˆ†`label`æ ‡ç­¾çš„å—?æ€ä¹ˆè·‘ç”Ÿæˆå¼é‡Œæ¥äº†?  

Okey,è¿™ä¸€èŠ‚ä¸»è¦å°±æ˜¯è®²è§£ä¸Šè¿°çš„ç–‘é—®,å¦‚æœæœ‰åŸºç¡€çš„åŒå­¦ï¼Œå¯ä»¥å…ˆè‡ªè¡Œæ¢ç´¢[ç›¸å…³ä»£ç ](./metrics.py)  

### 1. ç”Ÿæˆå¼çš„f1

#### 1.1 æ¨¡å‹æ¨ç†
- é¦–å…ˆ,å¯¹äºä¸€ä¸ªè¯„æµ‹æ•°æ®é›†,æˆ‘ä»¬é¦–å…ˆè¦æ„é€ å¼•å¯¼prompt,å³å¼•å¯¼llmç”Ÿæˆæˆ‘ä»¬æƒ³è¦çš„ç­”æ¡ˆã€‚å¯¹äºå·²æœ‰çš„æ•°æ®é›†,å¤§éƒ¨åˆ†éƒ½æä¾›äº†ç›¸åº”çš„prompt,åœ¨è‡ªå·±æ•°æ®é›†è¯„æµ‹æ—¶,ä¹Ÿå¯è‡ªè¡Œè®¾è®¡ã€‚ä»¥`multifieldqa_zh`ä¸ºä¾‹,å…¶å¼•å¯¼promptä¸º:
```
é˜…è¯»ä»¥ä¸‹æ–‡å­—å¹¶ç”¨ä¸­æ–‡ç®€çŸ­å›ç­”ï¼š\n\n{context}\n\nç°åœ¨è¯·åŸºäºä¸Šé¢çš„æ–‡ç« å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼Œåªå‘Šè¯‰æˆ‘ç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å­—è¯ã€‚\n\né—®é¢˜ï¼š{input}\nå›ç­”ï¼š
```
- ä¹‹å,å†æŒ‡å®šæ¨¡å‹çš„è¾“å…¥é•¿åº¦,åœ¨æ­¤ä¸»è¦æ˜¯è§„å®šæ¯æ¬¡é€è¿›æ¨¡å‹å¤šå°‘tokenæ•°,ä¸€èˆ¬ä¸ºäº†è¿½æ±‚æ€§èƒ½å¯ä»¥è®¾ç½®ä¸ºæ¨¡å‹æœ€å¤§é•¿åº¦,å¯ä»¥åœ¨ä¸‹è½½å¥½çš„æ¨¡å‹æ–‡ä»¶é‡Œé¢çš„`config.json`é‡Œé¢çš„"max_position_embeddings"æŸ¥è¯¢,ä¹Ÿå¯ä»¥ä¸è®¾ç½®ä½œä¸ºé»˜è®¤æœ€å¤§é•¿åº¦.ä½†æœ¬é¡¹ç›®è®¾ç½®ä¸ºäº†2048,ä¸»è¦ä¸ºäº†æ¼”ç¤ºä½¿ç”¨~  

- ä¹‹åå°±æ˜¯åˆ›å»ºmodelæ•´ä½“,åœ¨æ­¤æˆ‘å¯¹æ¨¡å‹æ•´ä½“åˆ›å»ºäº†ä¸€ä¸ªclass,å¤§å®¶å¯ä»¥å‚è€ƒå¯¹å…¶ä»–ä»»æ„çš„modelè¿›è¡Œç»„è£…:
```python
class BaseLLM:
    def __init__(self, path: str, model_name: str) -> None:
        self.path = path
        self.model_name = model_name

    def build_chat(self, tokenizer: str, prompt: str, model_name: str):
        pass

    def load_model_and_tokenizer(self, path: str, model_name: str, device):
        pass

    def post_process(self, response: str, model_name: str):
        pass

    def get_pred(self, data: list, max_length: int, max_gen: int, prompt_format: str, device, out_path: str):
        pass
```
- å‚æ•°è§£è¯»,build_chatä¸ºä½¿ç”¨æ¨¡å‹å›ºæœ‰çš„æ•°æ®åŠ è½½å½¢å¼,ä»¥`internlm2`ä¸ºä¾‹,å…¶ä¸º
```python
def build_chat(self, prompt):
        prompt = f'<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n'
        return prompt
```
- modelä¸tokenizerä¸ç”¨å¤šè¯´,åå¤„ç†æ ¹æ®modelçš„å½¢å¼é€‰æ‹©æ€§åˆ¤æ–­æ˜¯å¦éœ€è¦ï¼Œé‡ç‚¹è®²ä¸€ä¸‹`get_pred`å‡½æ•°ï¼š

```python
def get_pred(self, data, max_length, max_gen, prompt_format, device, out_path):
        model, tokenizer = self.load_model_and_tokenizer(self.path, device)
        for json_obj in tqdm(data):
            prompt = prompt_format.format(**json_obj)
            # åœ¨ä¸­é—´æˆªæ–­,å› ä¸ºä¸¤å¤´æœ‰å…³é”®ä¿¡æ¯.
            tokenized_prompt = tokenizer(prompt, truncation=False, return_tensors="pt").input_ids[0]
            if len(tokenized_prompt) > max_length:
                half = int(max_length/2)
                prompt = tokenizer.decode(tokenized_prompt[:half], skip_special_tokens=True)+tokenizer.decode(tokenized_prompt[-half:], skip_special_tokens=True)

            prompt = self.build_chat(prompt)

            input = tokenizer(prompt, truncation=False, return_tensors="pt").to(device)
            # è¡¨ç¤ºå–‚è¿›å»çš„tokensçš„é•¿åº¦
            context_length = input.input_ids.shape[-1]
            eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(["<|im_end|>"])[0]]

            output = model.generate(
                **input,
                max_new_tokens=max_gen,
                do_sample=False,
                temperature=1.0,
                eos_token_id=eos_token_id,
            )[0]
            
            pred = tokenizer.decode(output[context_length:], skip_special_tokens=True)
            pred = self.post_process(pred)
            
            with open(out_path, "a", encoding="utf-8") as f:
                json.dump({"pred": pred, "answers": json_obj["answers"], "all_classes": json_obj["all_classes"], "length": json_obj["length"]}, f, ensure_ascii=False)
                f.write('\n')
``` 

- æœ‰çš„åŒå­¦å¯èƒ½ä¼šé—®,ä¸ºå•¥è¦æ•´è¿™ä¹ˆä¸€å¤§ä¸²,ç›´æ¥ç”¨`model.chat()`ä¸é¦™å—?? 
- Okey!è¿™ä¸ªå‡½æ•°å°±å‘Šè¯‰äº†ä½ ç­”æ¡ˆã€‚åŸå› å°±åœ¨äºæˆªæ–­ç­–ç•¥,å¯¹äºæ¨¡å‹è€Œè¨€,å°¤å…¶æ˜¯åˆ¶å®šäº†è¾“å…¥çš„é•¿åº¦,å¦‚æœä½¿ç”¨é˜¶æ®µå‘½ä»¤åˆ™å…¶ä¼šåœ¨è¾“å…¥çš„æœ«å°¾è¿›è¡Œé˜¶æ®µ,ä½†ç”±äºå¼•å¯¼æ€§`prompt`çš„å­˜åœ¨,åœ¨`inputs`çš„ä¸¤ç«¯å‡æœ‰å…³é”®ä¿¡æ¯,æ•…éœ€è¦å¯¹ä¸¤ç«¯çš„ä¿¡æ¯è¿›è¡Œä¿ç•™,å¯¹ä¸­é—´éƒ¨ä½è¿›è¡Œæˆªæ–­æ“ä½œ,æ‰èƒ½æœ€å¤§é™åº¦åœ°æŠ±æŒè¾“å‡ºæ•ˆæœ!

> tips: get_predéƒ¨åˆ†,å¯ä»¥å‚è€ƒå„å¤§æ¨¡å‹å„è‡ªçš„`model`ç›¸å…³è„šæœ¬ä¸­çš„`chat`å‡½æ•°(`internlm2`åœ¨`modeling_internlm2.py`é‡Œé¢),ä¹Ÿå¯ä»¥æ›´å¥½çš„ç†è§£åŸå§‹æ–‡æœ¬è¾“å…¥ä¸ç»“æ„åŒ–æ¨¡å‹è¾“å‡ºã€‚

#### 1.2 ç»“æœè¯„æµ‹
ç›´æ¥showä¾‹å­:
```
"pred": "57081.86å…ƒ", "answers": "äººæ°‘å¸57081.86å…ƒã€‚"
```
- é¦–å…ˆ,ç»è¿‡æ•°æ®æ¸…æ´—ä¸`jieba`åˆ†è¯,å°†çŸ­å¥åˆ†ä¸ºè¯ç»„,ä»¥ç¤ºä¾‹æ–‡æœ¬ä¸ºä¾‹,ç»è¿‡åˆ†è¯ä¸å»æ‰æ ‡ç‚¹ç¬¦å·ç­‰æ“ä½œ,å¾—åˆ°ä¸‹åˆ—è¾“å‡º:
```
"pred": ['5708186', 'å…ƒ'], "answers": ['äººæ°‘å¸', '5708186', 'å…ƒ']"
```
å°†ä¸Šè¿°çš„ä¸¤ä¸ª"å¹²å‡€"çš„è¾“å‡ºé€å…¥`f1`è¯„åˆ†å‡½æ•°å¦‚ä¸‹:
```python
def f1_score(prediction, ground_truth, **kwargs):
    # Counterä»¥dictçš„å½¢å¼å­˜å‚¨å„ä¸ªå¥å­å¯¹åº”çš„è¯ä¸å…¶å¯¹åº”ä¸ªæ•°,&æ“ä½œç¬¦è¿”å›ä¸¤ä¸ªCounterä¸­å…±åŒçš„å…ƒç´ çš„é”®å€¼å¯¹
    common = Counter(prediction) & Counter(ground_truth)
    # æ˜¾ç¤ºpredictionä¸gtçš„å…±åŒå…ƒç´ çš„ä¸ªæ•°  
    num_same = sum(common.values())                       
    if num_same == 0:
        return 0
    # å³æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°é‡ä¸æ€»é¢„æµ‹æ ·æœ¬æ•°é‡çš„æ¯”å€¼
    precision = 1.0 * num_same / len(prediction)
    # æ¨¡å‹æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°é‡ä¸æ€»å®é™…æ ·æœ¬æ•°é‡çš„æ¯”å€¼         
    recall = 1.0 * num_same / len(ground_truth)           
    f1 = (2 * precision * recall) / (precision + recall)
    return f1
```
- é¦–å…ˆè®°å½•ä¸¤ä¸ªlistä¸­ç›¸åŒçš„å…ƒç´ ,å†ç»Ÿè®¡ç›¸åŒçš„å…ƒç´ çš„æ€»æ•°,æœ€ç»ˆå†æŒ‰ç…§precisionä¸recallçš„å®šä¹‰åˆ†åˆ«è®¡ç®—ç›¸åº”çš„åˆ†æ•°ã€‚  
- ç„¶åå°±å¾—åˆ°è¯¥ç»“æœçš„å¯¹åº”åˆ†æ•°å•¦,æœ€åå†å°†æ‰€æœ‰çš„ç»“æœå–å¹³å‡å€¼,å³å¾—åˆ°è¯¥`task`çš„`f1_score`

### ğŸ˜•ç–‘é—®
å½“ç„¶ï¼Œè¿™äº›åªæ˜¯åŸºç¡€çš„`metric`è¯„æµ‹æŒ‡æ ‡ï¼Œæˆ–è®¸ç»†å¿ƒçš„ä½ å·²ç»å‘ç°äº†ç›¸åº”çš„æ¼æ´ï¼Œæ¯”å¦‚åœ¨ä¸Šè¿°é¢„æµ‹ä¸­ï¼Œç›¸æ¯”è¾ƒçš„ç»“æœéƒ½æ˜¯ç»è¿‡äº†ç›¸åº”çš„è§„åˆ™æŠ½å–çš„ï¼Œå¦‚æœå‡ºç°äº†æ¯”å¦‚`answer`æ˜¯"å¦é—¨å¤§å­¦",è€Œ`pred`æ˜¯"ä¸æ˜¯å¦é—¨å¤§å­¦"/"å¦å¤§",åˆ™äºŒè€…çš„ç»“æœæŒ‰ç…§å½“å‰çš„è¯„åˆ†æŒ‡æ ‡åˆ™æœ‰å¤±åé¢‡ã€‚
    
å½“ç„¶,æ›´åŠ å‡†ç¡®çš„è¯„æµ‹metricä¹Ÿæ˜¯å­¦æœ¯ç•Œä¸€ç›´åŠªåŠ›çš„ç›®æ ‡,æœ¬é¡¹ç›®ä¹Ÿä¼šåŠæ—¶è·Ÿè¿›æ›´åŠ å…ˆè¿›çš„è¯„æµ‹ç­–ç•¥,ä¹Ÿæ¬¢è¿å¤§ä½¬PRï¼ï¼

## ğŸ˜†Get start!

### 1. get inference results
```python
python inference.py
```

### 2. get eval results
```python
python eval.py
```

## support metrics
1. f1 score
2. rouge-series/blue-series
3. accuracy

## Reference & Acknowledgment
[LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding](https://arxiv.org/abs/2308.14508)
